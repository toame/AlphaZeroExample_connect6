# -*- coding: utf-8 -*-
"""AlphaZeroExample_ipynb_V4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xE9SdS7PiCyMajCctBlB_Zu4UJYPdQAc
"""

# 環境構築
# pip3 install torch


# ゲームの実装 ここを他のドメインの実装に置き換えれば色々なゲームで動かせる
# 高速な言語でこの部分のみ実装すると手軽に高速化出来る(C++ならpybind11やBoost.Pythonを利用可)
import random
import numpy as np
import copy
BLACK, WHITE =1, -1 # 先手後手

class State:
    '''○×ゲームの盤面実装'''
    X, Y = 'ABCDEFGHIJKLMNOPQRS',  'abcdefghijklmnoprqs'
    C = {0: '_', BLACK: 'O', WHITE: 'X'}

    def __init__(self):
        self.prev_board = np.zeros((19, 19)) 
        self.board = np.zeros((19, 19)) # (x, y)
        self.color = 1
        self.win_color = 0
        self.record = []
        self.isTurn = True
        self.check = -1
        self.checked = []
        self.legal = np.zeros((19, 19)) 
        self.legal[9, 9] = True

    def action2str(self, a):
        return self.X[a // 19] + self.Y[a % 19]

    def str2action(self, s):
        return self.X.find(s[0]) * 19 + self.Y.find(s[1])

    def record_string(self):
        return ' '.join([self.action2str(a) for a in self.record])

    def __str__(self):
        # 表示
        s = '   ' + ' '.join(self.Y) + '\n'
        for i in range(19):
            s += self.X[i] + ' ' + ' '.join([self.C[self.board[i, j]] for j in range(19)]) + '\n'
        s += 'record = ' + self.record_string()
        return s

    def board_scan_sub(self, x, y, move_x, move_y):
        n = 1
        k = []
        for j in range(2):
          prev = len(k)
          for i in range(1, 6):
              if (x + move_x * i < 0 or x + move_x * i >= 19):
                break
              elif (y + move_y * i < 0 or y + move_y * i >= 19):
                break
              elif(self.board[x + move_x * i][y + move_y * i] == self.color and prev == len(k)):
                n += 1
              elif (self.board[x + move_x * i][y + move_y * i] == 0 and len(k) - prev <= 2):
                k.append((x + move_x * i) * 19 + (y + move_y * i))
              else:
                break
          move_x = -move_x
          move_y = -move_y
        return n, k

    def board_scan_sub2(self, x, y, move_x, move_y):
        max_n = 0
        max_k = []
        for j in range(-5, 1):
          n = 0
          k = []
          for l in range(6):
              i = j + l
              if (x + move_x * i < 0 or x + move_x * i >= 19):
                n = -9
                break
              elif (y + move_y * i < 0 or y + move_y * i >= 19):
                n = -9
                break
              elif(self.board[x + move_x * i][y + move_y * i] == -self.color):
                n = -9
                break
              elif(self.board[x + move_x * i][y + move_y * i] == self.color):
                n += 1
              elif (self.board[x + move_x * i][y + move_y * i] == 0 and len(k)<= 2):
                if(n == 0):
                  k.append((x + move_x * i) * 19 + (y + move_y * i))
                else:
                  k.insert(0,(x + move_x * i) * 19 + (y + move_y * i))
          if (max_n < n or (max_n == n and len(max_k) < len(k))):
            max_n = n
            max_k = k
        return max_n, max_k

    def board_scan_4_2(self, x, y):
        move_x = [1, 0, 1, -1]
        move_y = [1, 1, 0, 1]
        for arg in range(4):
            flag = True
            for j in range(-5, 1):
              for l in range(6):
                  i = j + l
                  if (x + move_x[arg] * i < 0 or x + move_x[arg] * i >= 19):
                    flag = False
                    break
                  elif (y + move_y[arg] * i < 0 or y + move_y[arg] * i >= 19):
                    flag = False
                    break
                  elif(self.board[x + move_x[arg] * i][y + move_y[arg] * i] == -self.color):
                    flag = False
                    break
                  elif((l == 0 or l == 5) and self.board[x + move_x[arg] * i][y + move_y[arg] * i] == 0):
                    continue
                  elif ((l == 1 or l == 2 or l == 3 or l == 4) and self.board[x + move_x[arg] * i][y + move_y[arg] * i] == 1):
                    continue
                  else:
                    flag = False
                    break;
              if (flag):
                return True
        return False


      
    def board_scan_sub3(self, x, y, move_x, move_y):
      sub_state = []
      for i in range(7):
        if (x + move_x * i < 0 or x + move_x * i >= 19):
          sub_state.append(-1)
        elif (y + move_y * i < 0 or y + move_y * i >= 19):
          sub_state.append(-1)
        elif(self.board[x + move_x * i][y + move_y * i] == -self.color):
          sub_state.append(-1)
        elif(self.board[x + move_x * i][y + move_y * i] == 0):
          sub_state.append(0)
        elif(self.board[x + move_x * i][y + move_y * i] == self.color):
          sub_state.append(1)
      return sub_state

    def board_scan_4_1(self, x, y):
      list4_1 = [[-1, 1, 1, 1, 1, 0, 0],\
            [-1, 1, 1, 1, 0, 1, 0],\
            [-1, 1, 1, 1, 0, 0, 1],\
            [-1, 1, 1, 0, 0, 1, 1],\
            [-1, 1, 1, 0, 1, 0, 1],\
            [-1, 1, 1, 0, 1, 1, 0],\
            [-1, 1, 0, 0, 1, 1, 1],\
            [-1, 1, 0, 1, 0, 1, 1],\
            [-1, 1, 0, 1, 1, 0, 1],\
            [-1, 1, 0, 1, 1, 1, 0],\
            [-1, 0, 0, 1, 1, 1, 1],\
            [-1, 0, 1, 0, 1, 1, 1],\
            [-1, 0, 1, 1, 0, 1, 1],\
            [-1, 0, 1, 1, 1, 0, 1],\
            [-1, 0, 1, 1, 1, 1, 0]]

      move_x = [1, 0, 1, -1]
      move_y = [1, 1, 0, 1]
      tmp = copy.deepcopy(self.board[x, y])
      self.board[x, y] = self.color
      for arg in range(4):
        for j in range(-5, 1):
          sub_state = self.board_scan_sub3(x + j * move_x[arg], y + j * move_y[arg], move_x[arg], move_y[arg])
          print(sub_state)
          if(sub_state in list4_1):
            self.board[x, y] = tmp
            return True
        self.board[x, y] = tmp
        return False
        
    def board_scan(self, x, y):
      move_x = [1, 0, 1, -1]
      move_y = [1, 1, 0, 1]
      for i in range(4):
        n, k = self.board_scan_sub(x, y, move_x[i], move_y[i])
        if (n >= 6):
          return True
      return False
      
    def isCheck(self):
      if (len(self.record) <= 5):
          return False
      if self.isTurn:
        act1 = self.record[-1]
        act2 = -1
      else:
        act1 = self.record[-3]
        act2 = self.record[-4]
      
      for j in range(2):
        #print("prev:" + str(self.action2str(act1)) + " isTurn:" + str(self.isTurn))
        x, y = act1 // 19, act1 % 19
        move_x = [1, 0, 1, -1]
        move_y = [1, 1, 0, 1]
        for i in range(4):
          n, k = self.board_scan_sub2(x, y, move_x[i], move_y[i])
          #print(str(n) + " " + str(k))
          if ((n >= 4 and len(k) >= 2 and not self.isTurn) or (n == 5 and len(k) >= 1)):
            self.check = k[0]
            return True
        if (act2 == -1):
          return False
        act1, act2 = act2, act1
      return False

    def isChecked(self):
      if (len(self.record) <= 4):
          return False
      if self.isTurn:
        act1 = self.record[-2]
        act2 = self.record[-3]
      else:
        act1 = self.record[-1]
        act2 = self.record[-2]
      #print("act1" + str(self.action2str(act1)) + " isTurn:" + str(self.isTurn))
      #print("act2" + str(self.action2str(act2)) + " isTurn:" + str(self.isTurn))
      self.color = -self.color
      for j in range(2):
        #print("check:" + str(self.action2str(act1)))
        x, y = act1 // 19, act1 % 19
        move_x = [1, 0, 1, -1]
        move_y = [1, 1, 0, 1]
        
        for i in range(4):
          n, k = self.board_scan_sub2(x, y, move_x[i], move_y[i])
          #print(str(n) + " " + str(k))
          if ((n == 4 and len(k) >= 2) or (n == 5 and len(k) >= 1)):
            #print(str(self.action2str(act1)) + " is checked")
            self.checked = k
            self.color = -self.color
            return True
        act1, act2 = act2, act1
      self.color = -self.color


      return False

    def play(self, action):
        # 行動で状態を進める関数
        # action は board 上の位置 (0 ~ 8) または行動系列の文字列
        if isinstance(action, str):
            for astr in action.split():
                self.play(self.str2action(astr))
            return self
        
        x, y = action // 19, action % 19
        self.prev_board = copy.deepcopy(self.board)
        self.board[x, y] = self.color
        for i in range(5):
          for j in range(5):
            xx = x + i - 2
            yy = y + j - 2
            if (0 <= xx and xx < 19 and 0 <= yy and yy < 19 and self.board[xx, yy] == 0):
              self.legal[xx, yy] = True
        self.legal[x, y] = False

        if self.board_scan(x, y):
          self.win_color = self.color
        
        self.record.append(action)
        if (self.isTurn):
          self.color = -self.color
        self.isTurn = not self.isTurn
        
        return self

    def terminal(self):
        # 終端状態かどうか返す
        return self.win_color != 0 or len(self.record) == 19 * 19

    def terminal_reward(self):
        # 終端状態での勝敗による報酬を返す
        return self.win_color if self.color == BLACK else -self.win_color

    def legal_actions(self):
        # 可能な行動リストを返す
        return [a for a in range(19 * 19) if self.legal[a // 19, a % 19] == True]

    def action_length(self):
        # 行動ラベルの総数(policyの出力サイズを決める)
        return 19 * 19

    def feature(self):
        # ニューラルネットに入力する状態表現を返す
        return np.stack([self.prev_board == self.color, self.prev_board == -self.color, self.board == self.color, self.board == -self.color]).astype(np.float32)

    def rotate(self, action, num):
        X = action // 19
        Y = action % 19
        if (num % 2 == 1):
            Y = 18 - Y
        if (num//2 == 0):
            return X * 19 + Y
        elif (num//2 == 1):
            prevX = X
            X = 18 - Y
            Y = prevX
            return X * 19 + Y
        elif (num//2 == 2):
            X = 18 - X
            Y = 18 - Y
            return (X * 19 + Y)
        elif (num//2 == 3):
            prevX = X
            X = Y
            Y = 18 - prevX
            return X * 19 + Y

    def rotatenp(self, l, num):
        a_1d = np.arange(19*19)
        a_1d = np.array(a_1d, dtype = 'float')
        for x in range(19*19):
            a_1d[x] = copy.deepcopy(l[State().rotate(x, num)])
        return a_1d

    

state = State().play('Bb')
print(state)
print('input feature')
print(state.feature())
state = State().play('Dd Aa Ab De Df Ac')
print('input feature')
print(state.feature())

# ニューラルネットの実装(PyTorch)
# AlphaZeroの論文のネットワーク構成を小さく再現

import torch
import torch.nn as nn
import torch.nn.functional as F
class Conv(nn.Module):
    def __init__(self, filters0, filters1, kernel_size, bn=False):
        super().__init__()
        self.conv = nn.Conv2d(filters0, filters1, kernel_size, stride=1, padding=kernel_size//2, bias=False)
        self.bn = None
        if bn:
            self.bn = nn.BatchNorm2d(filters1)

    def forward(self, x):
        h = self.conv(x)
        if self.bn is not None:
            h = self.bn(h)
        return h

class ResidualBlock(nn.Module):
    def __init__(self, filters):
        super().__init__()
        self.conv = Conv(filters, filters, 3, True)

    def forward(self, x):
        return F.relu(x + (self.conv(x)))

num_filters = 32
num_blocks = 4
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

class Net(nn.Module):
    '''ニューラルネット計算を行うクラス'''
    def __init__(self):
        super().__init__()
        state = State()
        self.input_shape = state.feature().shape
        self.board_size = self.input_shape[1] * self.input_shape[2]

        self.layer0 = Conv(self.input_shape[0], num_filters, 3, bn=True)
        self.blocks = nn.ModuleList([ResidualBlock(num_filters) for _ in range(num_blocks)])

        self.conv_p1 = Conv(num_filters, 4, 1, bn=True)
        self.conv_p2 = Conv(4, 1, 1)

        self.conv_v = Conv(num_filters, 4, 1, bn=True)
        self.fc_v = nn.Linear(self.board_size * 4, 1, bias=False)

    def forward(self, x):
        h = F.relu(self.layer0(x))
        for block in self.blocks:
            h = block(h)

        h_p = F.relu(self.conv_p1(h))
        h_p = self.conv_p2(h_p).view(-1, self.board_size)

        h_v = F.relu(self.conv_v(h))
        h_v = self.fc_v(h_v.view(-1, self.board_size * 4))

        # value(状態価値)にtanhを適用するので負け -1 ~ 勝ち 1
        return F.softmax(h_p, dim=-1), torch.tanh(h_v)

    def predict(self, state):
        # 探索中に呼ばれる推論関数
        self.eval()
        x = torch.from_numpy(state.feature()).unsqueeze(0)
        with torch.no_grad():
            p, v = self.forward(x)
        return p.cpu().numpy()[0], v.cpu().numpy()[0][0]

def show_net(net, state):
    '''方策 (p)　と　状態価値 (v) を表示'''
    print(state)
    p, v = net.predict(state)
    print('p = ')
    print((p *1000).astype(int).reshape((-1, *net.input_shape[1:7])))
    print('v = ', v)
    print()

# 学習前なのでランダムな出力を得る
#for i in range(1001):
#  Net().predict(state)
#  if (i % 200 == 0):
#      print(i)
#show_net(Net(), State())

# モンテカルロ木探索の実装

class Node:
    '''ある1状態の探索結果を保存するクラス'''
    def __init__(self, p, v):
        self.p, self.v = p, v
        self.n, self.q_sum = np.zeros_like(p), np.zeros_like(p)
        self.n_all, self.q_sum_all = 1, v / 2 # 事前分布(見解が分かれる点)

    def update(self, action, q_new):
        # 行動のスタッツを更新
        self.n[action] += 1
        self.q_sum[action] += q_new

        # ノード全体のスタッツも更新
        self.n_all += 1
        self.q_sum_all += q_new

def sumOfone(p):
  S = np.sum(p)
  p = p / S
  return p

# Commented out IPython magic to ensure Python compatibility.
import time, copy

class Tree:
    '''探索木を保持してモンテカルロ木探索を行うクラス'''
    def __init__(self, net):
        self.net = net
        self.nodes = {}
    
    def search(self, state, depth):
        # 終端状態の場合は末端報酬を返す
        #print(str(state.record[-1]) + " " +str(state.terminal()))
        if state.terminal():
            return state.terminal_reward()

        # まだ未到達の状態はニューラルネットを計算して推定価値を返す
        key = state.record_string()
        if key not in self.nodes:
            #print("currentstate:" + " " + state.record_string() + " turn:" + str(state.color)) 
            if state.isCheck():
                p = np.zeros(19 * 19)
                p[state.check] = 1
                v = state.color
                
                #print('p = ')
                #print((p *1000).astype(int).reshape((-1, *net.input_shape[1:7])))
                #print('v = ', v)
                #print()
            elif state.isChecked():
                p, v = self.net.predict(state)
                f = np.zeros(19 * 19)
                for move in state.checked:
                  f[move] = 1/len(state.checked)
                for i in range(19):
                  for j in range(19):
                    p = sumOfone(p * f)
                #print("last: " + str(state.action2str(state.record[-1])) + "next: " + str(state.action2str(state.checked)))
            else:
                p, v = self.net.predict(state)
            self.nodes[key] = Node(p, v)
            return v

        # 到達済みの状態はバンディットで行動を選んで状態を進める
        node = self.nodes[key]
        p = node.p
        if depth == 0:
            # ルートノード(現局面)では方策にノイズを加える
            p = 0.75 * p + 0.25 * np.random.dirichlet([0.15] * len(p))

        best_action, best_ucb = None, -float('inf')
        for action in state.legal_actions():
            n, q_sum = 1 + node.n[action], node.q_sum_all / node.n_all + node.q_sum[action]
            ucb = q_sum / n + 2.0 * np.sqrt(node.n_all) * p[action] / n # PUCBの式

            if ucb > best_ucb:
                best_action, best_ucb = action, ucb

        # 一手進めて再帰で探索
        state.play(best_action)
        if not state.isTurn:
          q_new = -self.search(state, depth + 1) # 1手ごとの手番交代を想定
        else:
          q_new = self.search(state, depth + 1)
        node.update(best_action, q_new)

        return q_new

    def think(self, state, num_simulations, kld_limit, temperature = 0, show=False):
        # 探索のエンドポイント
        cnt = 0
        if show:
            print(state)
        check = 0
        start, prev_time = time.time(), 0
        while cnt < num_simulations:
            cnt += 1
            self.search(copy.deepcopy(state), depth=0)
            if (num_simulations - cnt <= 1):
              r = self.rootpv(state)
              root, pv = self.nodes[state.record_string()], self.pv(state)
              if(check == 1):
                  kld = KLD(prev, root.n)
                  if (kld > kld_limit):
                    num_simulations += 50
                    prev = copy.deepcopy(root.n)
                  else:
                     break
              else:
                num_simulations += 50
                check = 1
                prev = copy.deepcopy(root.n)

            # 1秒ごとに探索結果を表示
            if show:
                tmp_time = time.time() - start
                if int(tmp_time) > int(prev_time):
                    prev_time = tmp_time
                    root, pv = self.nodes[state.record_string()], self.pv(state)
                    if (len(pv) > 0):
                        print('%.2f sec. best %s. q = %.4f. n = %d / %d. pv = %s'
                               % (tmp_time, state.action2str(pv[0]), root.q_sum[pv[0]] / root.n[pv[0]],
                                root.n[pv[0]], root.n_all, ' '.join([state.action2str(a) for a in pv])))
        #print(num_simulations)
        if show:
          r = self.rootpv(state)
          root, pv = self.nodes[state.record_string()], self.pv(state)
          for k in range(len(r)):
              print('%s. q = %.4f. n = %d / %d'
                     % (state.action2str(r[k]), root.q_sum[r[k]] / root.n[r[k]], root.n[r[k]], root.n_all))
        #  訪問回数で重みつけた確率分布を返す
        #print(state.record_string())
        n = root = self.nodes[state.record_string()].n + 0.01
        n = (n / np.max(n)) ** (1 / (temperature + 1e-8)) 
        return n / n.sum(), num_simulations

    def pv(self, state):
        # 最善応手列（読み筋）を返す
        s, pv_seq = copy.deepcopy(state), []
        while True:
            key = s.record_string()
            if key not in self.nodes or self.nodes[key].n.sum() == 0:
                break
            best_action = sorted([(a, self.nodes[key].n[a]) for a in s.legal_actions()], key=lambda x: -x[1])[0][0]
            pv_seq.append(best_action)
            s.play(best_action)
        return pv_seq

    def rootpv(self, state):
        s, pv_seq = copy.deepcopy(state), []
        action = []
        key = s.record_string()
        for k in range(len(sorted([(a, self.nodes[key].n[a]) for a in s.legal_actions()], key=lambda x: -x[1]))):
            action.append(int([(a, self.nodes[key].n[a]) for a in s.legal_actions()][k][0]))
        return action

'''
def checksearch(state, depth, checkcolor):
  if (depth == 0 and state.color == checkcolor and state.isChecked()):
    return False, -1
  if (depth >= 9):
    return False
  if (state.color != checkcolor and not state.isChecked()):
    return False
  if(state.terminal()):
    return True
  if (state.color == checkcolor and state.isCheck()):
    show_net(net, state)
    return True

  

  if (state.color != checkcolor):
    state.isChecked()
    state.play(state.checked[0])
    return checksearch(state, depth + 1, checkcolor)
  
  for action in state.legal_actions():
    x, y = action // 19, action % 19
    isAction = False
    for i in range(-1, 2):
      if ((x + i) < 0 or (x + i) >= 19):
        continue
      for j in range(-1, 2):
        if ((y + j) < 0 or (y + j) >= 19 ):
          continue
        if (checkcolor == state.board[x + i, y + j]):
          isAction = True
    if (not isAction):
      continue

    move_x = [1, 0, 1, -1]
    move_y = [1, 1, 0, 1]

    max_n = 0
    next_state = copy.deepcopy(state)
    next_state.play(action)
    for i in range(4):
      if (not next_state.isTurn):
        next_state.color = -next_state.color
      n, k = next_state.board_scan_sub2(x, y, move_x[i], move_y[i])
      if (not next_state.isTurn):
        next_state.color = -next_state.color
      max_n = max(max_n, n)
      #print(str(n) + " " + str(k))
    if (max_n <= 2):
      continue
    if (checksearch(next_state, depth + 1, checkcolor)):
      if (depth == 0):
        return True, action
      else:
        return True
  if(depth == 0):
    return False, -1
  return False

state = State().play("Jj Ii Jh Jk Jl Hj Ig Kj Lj Kg Ih Lf Gk If Hh Ie Ij Mj Kh Gh Lh")
#show_net(net,state)
isCheck, action = checksearch(state, 0, state.color)
print(state.action2str(action))
'''

def test(ori_state):
  for i in range(19):
    for j in range(19):
       state = copy.deepcopy(ori_state)
       state.play(i * 19 + j)
       if (state.board_scan_4_1(i, j)):
         print(1, end = '')
       elif (state.board_scan_4_2(i, j)):
         print(2, end = '')
       else:
         print(0, end = '')
    print()

import math
def KLD(distribution1, distribution2):
  kld = 0
  sum1 = 0
  sum2 = 0

  for i in range(len(distribution1)):
      sum1 += distribution1[i]
      sum2 += distribution2[i]
 
  for i in range(len(distribution1)):
      kld += (distribution2[i]/sum2) * math.log(((distribution2[i]/sum2) + 0.00001)/((distribution1[i]/sum1)+ 0.00001))
  #print(kld)
  return kld

# 初期ネットワークで探索を行う
state = State()
tree=Tree(Net())
tree = Tree(Net())
#tree.think(State(), 1000, 0.5, show=True)
#state = State().play("Jj Ii Ik Ij Hj Kj Gj Ji Jk Jm Hm")
#for i in range(10000):
#  Net().cuda().predict(state)
#  if (i % 1000 == 0):
#      print(i)

# ニューラルネットの学習

import torch.optim as optim
import random
batch_size = 64
num_epochs = 1

def gen_target(ep):
    '''ニューラルネットの学習用 input, targets を生成'''
    turn_idx = np.random.randint(len(ep[0]))
    state = State()
    for a in ep[0][:turn_idx]:
        state.play(a)
    v = ep[1]
    return state.feature(), ep[2][turn_idx], [v if ((turn_idx + 1) % 4)//2 == 0 else -v]

def train(episode, net):
    #net = Net()
    episodes = random.sample(episode, len(episode))
    optimizer = optim.SGD(net.parameters(), lr=5e-5, weight_decay=1e-4, momentum=0.75)
    for epoch in range(num_epochs):
        p_loss_sum, v_loss_sum = 0, 0
        net.train()
        for i in range(0, len(episodes)//2, batch_size):
            x, p_target, v_target = zip(*[gen_target(episodes[np.random.randint(len(episodes))]) for j in range(batch_size)])
            x = torch.FloatTensor(np.array(x))
            p_target = torch.FloatTensor(np.array(p_target))
            v_target = torch.FloatTensor(np.array(v_target))
            p, v = net(x)
            p_loss = torch.sum(-p_target * torch.log(p))
            v_loss = torch.sum((v_target - v) ** 2)

            p_loss_sum += p_loss.item()
            v_loss_sum += v_loss.item()

            optimizer.zero_grad()
            (p_loss + v_loss).backward()
            optimizer.step()
            print('p_loss %f v_loss %f' % (p_loss_sum / (i + batch_size), v_loss_sum / (i + batch_size)))
        for param_group in optimizer.param_groups:
            param_group['lr'] *= 0.8
        #print('p_loss %f v_loss %f' % (p_loss_sum / len(episodes), v_loss_sum / len(episodes)))
    #print('p_loss %f v_loss %f' % (p_loss_sum / len(episodes), v_loss_sum / len(episodes)))
    return net

'''
net3 = Net()
for i in range(4):
  net3 = train(episodes, net3)
  '''

#  ランダムプレーヤーとの対戦実験

def vs_random(net, n=100):
    results = {}
    for i in range(n):
        first_turn = i % 2 == 0
        turn = first_turn
        state = State()
        isTurnChange = True
        while not state.terminal():
            if turn:
                p, _ = net.predict(state)
                action = sorted([(a, p[a]) for a in state.legal_actions()], key=lambda x:-x[1])[0][0]
            else:
                action = np.random.choice(state.legal_actions())
            state.play(action)
            if isTurnChange:
              turn = not turn
            isTurnChange = not isTurnChange
        r = state.terminal_reward() if turn else -state.terminal_reward()
        results[r] = results.get(r, 0) + 1
    return results

def vs_random_s(net, n=100, num_simulations = 50):
    results = {}
    for i in range(n):
        first_turn = i % 2 == 0
        turn = first_turn
        state = State()
        isTurnChange = True
        while not state.terminal():
            if turn:
                tree = Tree(net)
                p = tree.think(state, num_simulations, 0.2)
                action = sorted([(a, p[a]) for a in state.legal_actions()], key=lambda x:-x[1])[0][0]
            else:
                action = np.random.choice(state.legal_actions())
            state.play(action)
            turn = not turn
        r = state.terminal_reward() if turn else -state.terminal_reward()
        results[r] = results.get(r, 0) + 1
        if (i % 100 == 0):
            print('vs_random = ', sorted(results.items()))
    return results
def vs_net(net1, net2, n=10, num_simulations = 100):
    results = {}
    for i in range(n):
        first_turn = i % 2 == 0
        turn = first_turn
        state = State()
        isTurnChange = True
        while not state.terminal():
            if turn:
                tree = Tree(net1)
                p, tmp = tree.think(state, num_simulations, 0.08, 0.4)
                #print (p)
                action = sorted([(a, p[a]) for a in state.legal_actions()], key=lambda x:-x[1])[0][0]
            else:
                tree = Tree(net2)
                p, tmp = tree.think(state, num_simulations, 0.08, 0.4)
                #print (p)
                action = sorted([(a, p[a]) for a in state.legal_actions()], key=lambda x:-x[1])[0][0]
            state.play(action)
            print(state.record_string())
            if isTurnChange:
              turn = not turn
            isTurnChange = not isTurnChange
        r = state.terminal_reward() if turn else -state.terminal_reward()
        results[r] = results.get(r, 0) + 1
        print('vs_net = ', sorted(results.items()))
    return results

#net191210_2132 = net
import pickle
f = open('episodes.binaryfile','rb')
episodes = pickle.load(f)

f = open('episodes2.binaryfile','rb')
episodes2 = pickle.load(f)

f = open('net2.binaryfile','rb')
net = pickle.load(f)
#'''
#net11220550 = net
#print('vs_net = ', sorted(vs_net(net,net191210_2132,10).items()))
#print('vs_random = ', sorted(vs_random_s(net, 1000, 50).items()))

#nets = []
#net = train(episodes, net)
#nets.append(net)
#net = train(episodes2, net)
#nets.append(net)
#net = train(episodes, net)
#net = train(episodes2, net)

'''
#net = Net()
show_net(net, State().play("Jj Ii Jh Jk Jl Hj Ig Kj Lj Kg Ih Lf Gk If Hh Ie Ij Mj Kh Gh Lh Jg"))
for i in range(8):
  net = train(episodes, net)
  show_net(net, State().play("Jj Ii Jh Jk Jl Hj Ig Kj Lj Kg Ih Lf Gk If Hh Ie Ij Mj Kh Gh Lh Jg"))
  '''

# AlphaZeroのアルゴリズムメイン
try:
    import time
    num_games = 100
    num_train_steps = 10
    num_simulations = 100
    print (len(episodes))
    #for i in range(len(episodes)//2):
    #  episodes.pop(0)
    result_distribution = {1:0, 0:0, -1:0}
    #print('vs_random = ', sorted(vs_random(net, 100).items()))
    sum_simulation = 0
    sum_cnt = 0
    for g in range(num_games):
        # 1対戦のエピソード生成
        record, p_targets = [[],[],[],[],[],[],[],[]], [[],[],[],[],[],[],[],[]]
        state = State()
        tree = Tree(net)
        temperature = 0.9 # 探索結果から方策のtargetを作るときの温度
        print(g + 1, ' ', end='')
        while not state.terminal() and len(record[0]) <= 85:
            if(((g % 2) * 2 - 1) == state.color):
              #print("last", end = " ")
              tree = Tree(net)
            else:
              #print("other", end = " ")
              tree = Tree(net)
            p_target, tmp = tree.think(state, num_simulations, 0.1, temperature)
            sum_simulation += tmp
            sum_cnt += 1
            action = np.random.choice(np.arange(len(p_target)), p=p_target)
            # 行動をランダムに選んで進める
            if (len(state.record) == 0):
              action = state.str2action("Jj")
            #elif(not state.isCheck()):
            #  p_target = center(p_target, state, 2)
            #  action = np.random.choice(np.arange(len(p_target)), p=p_target)
            else:
              action = np.random.choice(np.arange(len(p_target)), p=p_target)
            prev_state = copy.deepcopy(state)
            state.play(action)
            print(State().action2str(action), end=' ',flush =True)
            for i in range(8):
              record[i].append(State().rotate(action, i))
              p_targets[i].append(State().rotatenp(p_target, i))
              #print(record[i])
              #print(p_targets[i])
            temperature *= 0.95
            temperature = max(0.4, temperature)
        show_net(net, prev_state)
        # 先手視点の報酬
        reward = state.win_color
        result_distribution[reward] += 1
        print("reward" + str(reward))
        for i in range(8):
            episodes.append((record[i], reward, p_targets[i]))
        # ニューラルネットの学習
        if (g + 1) % num_train_steps == 0:
            print(sum_simulation/sum_cnt)
            sum_simulation = 0
            sum_cnt = 0
            # 先手勝、引き分け、後手勝ちの割合を表示
            print('generated = ', sorted(result_distribution.items()))
            net = train(episodes, net)
            #nets.append(net)
            net = train(episodes2, net)
            #nets.append(net)
            for i in range(len(episodes)//100):
              episodes.pop(0)
            #print('vs_random = ', sorted(vs_random(net, 100).items()))
            #print('vs_net = ', sorted(vs_net(net, net11220550).items()))
    print('finished')
except KeyboardInterrupt:
    print("Ctrl+Cで停止しました")
    f = open('episodes.binaryfile','wb')
    pickle.dump(episodes,f)
    f.close

    f = open('episodes2.binaryfile','wb')
    pickle.dump(episodes2,f)
    f.close


    f = open('net2.binaryfile','wb')
    pickle.dump(net,f)
    f.close

    print("保存しました")


